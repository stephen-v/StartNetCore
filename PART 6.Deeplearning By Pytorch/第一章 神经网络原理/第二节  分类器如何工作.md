#一、激活函数

在第一节中我们了解到，神经元不是单纯线性的，线性函数是只要有输入$x$，必定会有一个输出$y$与之对应，而神经元接收到信号不会马上做出响应，它会等待输入信号强度增大到超过阈值才会有输出，这就好比往杯子中倒水，只有水超过杯子的上边缘才会溢出来。所以，实际应用中我们会加入一个激活层，该激活层实际上是一些激活函数。在下图中，假设有三个输入，激活函数为$g(x)$，则最后的输出是$y=g(\alpha_{_{_1}}x_{{_1}}+\alpha_{_{_2}}x_{{_2}}+\alpha_{_{_3}}x_{{_3}})$，第一个神经元计算输入的加权和，然后通过第二个神经元激活函数的作用输出$y$。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-17-15-57-50.png'>
<p style='text-align:center'>图1.2.1</p>
</div>

在神经网络中，激活函数的主要作用是引入非线性因素，解决线性模型不能解决的问题。上图中，为什么一定要做非线性变换而不直接输出$y=\alpha_{_{_1}}x_{{_1}}+\alpha_{_{_2}}x_{{_2}}+\alpha_{_{_3}}x_{{_3}}$呢？那是因为，线性输出$y$（也叫做预测值）会随着输入值的增大而无限增大，但有时候我们并不需要那么大的实际值，比如在做分类的时候，我们仅仅需要知道的是在某个阈值上下，$y$是为1（被激活）还是为0（未激活），通过判断1或者0就可以直接做出预测。另外，如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。在上一节中，我们遇到一个特殊的情况，异或函数会让线性分类器无法把输入正确的归类，我们的解决办法是使用两个线性分类器，也许还有另一种办法，那就是使用圆把中间两个值圈起来，这种方法，其实就需要非线性变换。

激活函数在神经网络中的使用频率非常高，我们来看看常用的几个。
<b>阶跃函数</b>
为了模仿真正的生物神经元收到刺激后作出反应，我们在数学中首先想到的是阶跃函数。阶跃函数是在输入值未达到阈值后，输出一直为0，当输入达到阈值，输出值做突然的跳转，但这种冷冰冰，赤裸裸的尖锐边缘不太符合实际的应用，所以使用较少。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-10-40-25.png'>
<p style='text-align:center'>图1.2.2</p>
</div>

<b>Sigmoid函数</b>
相对于阶跃函数的尖锐突出，Sigmoid函数显得要平滑很多，无论它的输入值是无穷大还是无穷小，输出值范围均为$(0,1)$，很适合二分类问题，且输出的取值也很容易和概率取值联系起来。但是，Sigmoid也存在一些缺点，比如函数饱和使梯度消失，该函数的输出不是零中心的，这两个问题（后面梯度章节会讲解）在很多时候限制了它的使用。它的函数形式为：$$Sigmoidx=\frac{1}{1+e^{-x}}$$
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-11-13-09.png'>
<p style='text-align:center'>图1.2.3</p>
</div>


<b>Tanh函数</b>
双曲正切函数跟Sigmoid函数很像，但是输出值范围扩大了，从-1到1，它也存在函数饱和问题，不过输出是零中心的，使得Tanh函数比Sigmoid函数更受欢迎。函数形式为$$Tanhx=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$

<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-12-26-21.png'>
<p style='text-align:center'>图1.2.4</p>
</div>

<b>Softmax函数</b>
Softmax函数用于多分类网络中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，在最后输出时，选取概率最大的值作为预测值，从而来进行多分类！假设有一个数组，V，Vi表示V中的第i个元素，那么这个元素的softmax值就是：$$S_{_{i}}=\frac{e^{i}}{\sum_{j}^{e^{j}}}$$
用下图更清晰的表示Softmax函数的工作原理。三个输入经过Softmax的处理后，得到不同概率值，一般来说我们选取概率最大的值作为预测目标。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-17-15-07.png'>
<p style='text-align:center'>图1.2.4</p>
</div>

<b>Relu函数</b>
中文叫线性整流函数，又称修正线性单元，以斜坡函数为代表，当输入小于0时，输出为0，当输入大于0时，输出等于输入。该函数也是仿效生物神经元，最大的特点是可以避免梯度爆炸和梯度消失等问题，计算过程简单，因此，在深度神经网络中应用广泛。它的函数形式为:$$Relux=max(0,x)$$
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-14-18-51.png'>
<p style='text-align:center'>图1.2.6</p>
</div>

#二、一个简单线性分类器工作原理
既然线性分类器是一条线性线段，在数学中，我们可以把线段表示为$y=\alpha x+\beta $，$x$代表宽度，$y$代表长度，$\alpha$和$\beta$分别为直线的斜率和截距。而在神经网络中，我们把$x$称为输入，$y$称为输出，或者叫预测值，$\alpha$称为权重，$\beta$称为随机扰动项。如下图1.2.2所示的线性分类器。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-17-11-07-30.png'>
<p style='text-align:center'>图1.2.7</p>
</div>








参考文献：
1、《Python神经网络编程》
2、https://www.zhihu.com/question/22334626
3、https://zhuanlan.zhihu.com/p/21462488
4、https://www.zhihu.com/question/23765351