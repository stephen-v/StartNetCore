#一、激活函数

在第一节中我们了解到，神经元不是单纯线性的，线性函数是只要有输入$x$，必定会有一个输出$y$与之对应，而神经元接收到信号不会马上做出响应，它会等待输入信号强度增大到超过阈值才会有输出，这就好比往杯子中倒水，只有水超过杯子的上边缘才会溢出来。所以，实际应用中我们会加入一个激活层，该激活层实际上是一些激活函数。在下图中，假设有三个输入，激活函数为$g(x)$，则最后的输出是$y=g(\alpha_{_{_1}}x_{{_1}}+\alpha_{_{_2}}x_{{_2}}+\alpha_{_{_3}}x_{{_3}})$，第一个神经元计算输入的加权和，然后通过第二个神经元激活函数的作用输出$y$。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-17-15-57-50.png'>
<p style='text-align:center'>图1.2.1</p>
</div>

在神经网络中，激活函数的主要作用是引入非线性因素，解决线性模型不能解决的问题。上图中，为什么一定要做非线性变换而不直接输出$y=\alpha_{_{_1}}x_{{_1}}+\alpha_{_{_2}}x_{{_2}}+\alpha_{_{_3}}x_{{_3}}$呢？那是因为，线性输出$y$（也叫做预测值）会随着输入值的增大而无限增大，但有时候我们并不需要那么大的实际值，比如在做分类的时候，我们仅仅需要知道的是在某个阈值上下，$y$是为1（被激活）还是为0（未激活），通过判断1或者0就可以直接做出预测。另外，如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。在上一节中，我们遇到一个特殊的情况，异或函数会让线性分类器无法把输入正确的归类，我们的解决办法是使用两个线性分类器，也许还有另一种办法，那就是使用圆把中间两个值圈起来，这种方法，其实就需要非线性变换。

激活函数在神经网络中的使用频率非常高，我们来看看常用的几个。
<b>阶跃函数</b>
为了模仿真正的生物神经元收到刺激后作出反应，我们在数学中首先想到的是阶跃函数。阶跃函数是在输入值未达到阈值后，输出一直为0，当输入达到阈值，输出值做突然的跳转，但这种冷冰冰，赤裸裸的尖锐边缘不太符合实际的应用，所以使用较少。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-10-40-25.png'>
<p style='text-align:center'>图1.2.2</p>
</div>

<b>Sigmoid函数</b>
相对于阶跃函数的尖锐突出，Sigmoid函数显得要平滑很多，无论它的输入值是无穷大还是无穷小，输出值范围均为$(0,1)$，很适合二分类问题，且输出的取值也很容易和概率取值联系起来。但是，Sigmoid也存在一些缺点，比如函数饱和使梯度消失，该函数的输出不是零中心的，这两个问题（后面梯度章节会讲解）在很多时候限制了它的使用。它的函数形式为：$$Sigmoidx=\frac{1}{1+e^{-x}}$$
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-11-13-09.png'>
<p style='text-align:center'>图1.2.3</p>
</div>


<b>Tanh函数</b>
双曲正切函数跟Sigmoid函数很像，但是输出值范围扩大了，从-1到1，它也存在函数饱和问题，不过输出是零中心的，使得Tanh函数比Sigmoid函数更受欢迎。函数形式为$$Tanhx=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$

<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-12-26-21.png'>
<p style='text-align:center'>图1.2.4</p>
</div>

<b>Softmax函数</b>
Softmax函数用于多分类网络中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，在最后输出时，选取概率最大的值作为预测值，从而来进行多分类！假设有一个数组，V，Vi表示V中的第i个元素，那么这个元素的softmax值就是：$$S_{_{i}}=\frac{e^{i}}{\sum_{j}^{e^{j}}}$$
用下图更清晰的表示Softmax函数的工作原理。三个输入经过Softmax的处理后，得到不同概率值，一般来说我们选取概率最大的值作为预测目标。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-17-15-07.png'>
<p style='text-align:center'>图1.2.4</p>
</div>

<b>Relu函数</b>
中文叫线性整流函数，又称修正线性单元，以斜坡函数为代表，当输入小于0时，输出为0，当输入大于0时，输出等于输入。该函数也是仿效生物神经元，最大的特点是可以避免梯度爆炸和梯度消失等问题，计算过程简单，因此，在深度神经网络中应用广泛。它的函数形式为:$$Relux=max(0,x)$$
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-14-18-51.png'>
<p style='text-align:center'>图1.2.6</p>
</div>

#二、一个简单线性分类器工作原理
###<b>残差公式</b>
既然线性分类器可以用一条线性线段表示，即在数学中线性线段为$y=\alpha x+\beta $，$x$代表宽度，$y$代表长度，$\alpha$和$\beta$分别为直线的斜率和截距。而在神经网络中，我们把$x$称为输入，$y$称为输出，或者叫预测值，$\alpha$称为权重，$\beta$称为随机扰动项。如下图1.2.2所示的线性分类器。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-24-17-09-49.png'>
<p style='text-align:center'>图1.2.7</p>
</div>

这样我们就把问题转化为分析两个变量$x$和$y$之间的关系，即分析$y=\alpha x+\beta$，比如一个程序员的收入$(x)$与支出$(y)$之间的关系，房子大小$(x)$与价格$(y)$之间的关系等等。

如上1.2.7的数据分布图，我们大概可以评估一下它们之间是一种线性的关系，但这种线性关系可以是多种，怎么才是最好的模型（分类器）呢，那就要抓住关键点，即找出预测值和真实值之间的差异。下图1.2.8可视化了这种差异，图中的$Δyi$称为残差，是预测值和真实值之差，用公式表示即为：$Δy_{_i}=\alpha x_{_i}+\beta-y_{_i}$，在这个等式中$\alpha x_{_i}$为模型预测值，$y_{_i}$为某一样本真实值，在整个等式中，$\alpha$和$\beta$未知，我们的目标就是要求解最优$\alpha$、$\beta$值，然后做出最接近真实值的预测。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-24-17-56-59.png'>
<p style='text-align:center'>图1.2.8</p>
</div>

为了便于计算$\alpha$和$\beta$，我们一般使用残差平方，这样在对$\alpha$和$\beta$求偏导的时候不至于出现0的情况。多样本的情况下，上面的公式可以表示为$$\varepsilon _{i}^2=\frac{1}{2}\sum_{i=1}^{N}(\alpha_{i} x_{i}+\beta-y_{i})^2$$
这就是有名的残差公式，只要能够使得残差达到最小值，那么我们就能得到最优的$\alpha$和$\beta$值，从而得到更精确的预测值。

###<b>梯度下降法</b>
现在我们先忽略多样本的情况，把这个让人凌乱的求和符号$\sum_{i=1}^{N}$给去掉，看看一个样本的情况。假设某个样本的残差为$\epsilon =(\alpha x+\beta-y)^2$（令$\epsilon=\varepsilon_{i}^2$），做$\alpha$和Є的坐标图如下1.2.9，可以看到该曲线存在一个极小值点，只有在该点，Є的值才为最小，如何找到函数极小值对应的$\alpha$呢，我们对$\alpha$求偏导！高数告诉我们偏导数即为某点的斜率，下图中$\alpha_{_1}$点的斜率用红色的箭头表示，该线段在$\alpha=\alpha_{_1}$点处与曲线相切，箭头所指方向则为导数的反方向，仔细看看这个图，只要我们沿着导数相反的方向移动$\alpha$（对于$\beta$来说也是一样），就会找到极小值点了！
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-25-16-49-15.png'>
<p style='text-align:center'>图1.2.9</p>
</div>

对于上面的偏导$\frac{\partial \epsilon }{\partial \alpha }$我们也称为梯度，残差方程我们也称为损失函数，刚才讲的沿着导数的反方向向下移动寻找曲线极小值点的方法就叫梯度下降算法。在进行移动的时候需要考虑每次移动的速度，我们把这个速度叫做学习率，用$\eta$表示，通过梯度和学习率我们可以得到$\alpha$的新公式为：$$\alpha_{_1}=\alpha_{_0}-\eta \frac{\partial \epsilon }{\partial \alpha}$$
延伸为一般的情况为：$$\alpha_{_n+1}=\alpha_{_n}-\eta \frac{\partial \epsilon }{\partial \alpha}$$

由此可见，学习率的增大可以加速逼近极小值的情况，但是如果在快要到达函数的底端的时候，需要减小学习率，以免出现Є不断增大或者不停摆动的情况，所以我们应该果断选取一个较小的学习率，以保证Є能减少到一个稳定的值(我们称为收敛converge)。

下面用一些具体值来说明梯度下降算法是如何工作的,首先为了简化计算，假设$\beta=0$，当前样本值为(x,y)=(1,0)，则损失函数表示为$\epsilon=(\alpha -1)^2$，参数$\alpha$的梯度为$\bigtriangledown =\frac{\partial \epsilon }{\partial \alpha }=2 (\alpha -1)$，那么使用梯度下降算法每次对参数$\alpha$的更新公式为$\alpha_{_n+1}=\alpha_{_n}-\eta \bigtriangledown n$，设置参数的初始值为5，学习率为0.3，那么这个优化过程可以总结为下表1.2.1。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-28-14-10-00.png'>
<p style='text-align:center'>表1.2.1</p>
</div>

可以看到，经过了10次迭代滞后，参数$\alpha$的值变成了1.00043，，这个和参数最优值1已经比较接近了。虽然这里给出的是一个非常简单的样例，但是神经网络的优化过程也是可以类推的。神经网络的优化过程可以分为两个阶段，第一阶段先通过前向传播算法计算得到预测值，即计算$\alpha x_{_i}+\beta$，并将预测值和真实值做对比得出两者之间的差距，即得出残差方程。然后第二个阶段通过反向传播算法计算损失函数对每一个参数的梯度，再根梯度和学习率使用梯度下降算法更新每一个参数。

需要注意的是，梯度下降算法并不能保证被优化的函数达到全局最优解。如下图所示，图中给出的函数就有可能只能得到局部最优解，而不是全局最优解。在小红点处，损失函数的偏导为0，于是参数就停止更新了。只有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-28-16-34-12.png'>
<p style='text-align:center'>表1.2.1</p>
</div>

#三、实例
在上一节的第四章中，我们模拟了一段神经元如何换算单位升和加仑的场景，从一开始，$\alpha$就是已知值，神经元通过预测值和真实值的比较不断的优化$\alpha$。但实际上，这并不是真实的工作场景，因为计算机一开始并不知道$\alpha$到底是什么，具体值是多少，只有通过对训练数据的不断学习，缩小真实值和预测值的差异，不断优化$\alpha$值，才能得到最接近真实值的预测。总的来说，把预测值和真实值做对比，并减小它们之间的差异，是最关键的一步。这就要求我们必须给出已知的两类物品的特征数据，比如豆子的长宽、颜色数据，且是黄豆或蚕豆等真实值，否则，计算机无法进行分类工作。

下面我们用经典的鸢尾花数据集来做一次简单的训练，如下表，有两种类别的鸢尾花各十条数值类型数据（注：原数据集有100多条，分三类，这里省略一些分两类），数据集包括4个属性，分别是萼片长度，宽度，花瓣长度，宽度。四个特征进行分类，也就是说分类器有四个输入，并采用最简单的阶跃函数做激活函数，再规定分类器输出1表示Iris-setosa(山鸢尾)品种，输出0表示Iris-Versicolour(变色鸢尾)。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-24-16-03-41.png'>
<p style='text-align:center'>表1.2.1</p>
</div>










参考文献：
1、《Python神经网络编程》
2、https://www.zhihu.com/question/22334626
3、https://zhuanlan.zhihu.com/p/21462488
4、https://www.zhihu.com/question/23765351
5、https://blog.csdn.net/ppn029012/article/details/8775597
6、《Tensorflow实战Google深度学习框架》