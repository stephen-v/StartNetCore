

<!-- TOC -->

- [一、激活函数](#一激活函数)
- [二、线性回归与梯度下降](#二线性回归与梯度下降)
- [三、矩阵乘法](#三矩阵乘法)

<!-- /TOC -->

<a id="markdown-一激活函数" name="一激活函数"></a>
# 一、激活函数

在第一节中我们了解到，神经元不是单纯线性的，线性函数是只要有输入$x$，必定会有一个输出$y$与之对应，而神经元接收到信号不会马上做出响应，它会等待输入信号强度增大到超过阈值才会有输出，这就好比往杯子中倒水，只有水超过杯子的上边缘才会溢出来。所以，实际应用中我们会加入一个激活层，该激活层实际上是一些激活函数。在下图中，假设有三个输入，激活函数为$g(x)$，则最后的输出是$y=g(\alpha_{_{_1}}x_{{_1}}+\alpha_{_{_2}}x_{{_2}}+\alpha_{_{_3}}x_{{_3}})$，第一个神经元计算输入的加权和，然后通过第二个神经元激活函数的作用输出$y$。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-17-15-57-50.png'>
<p style='text-align:center'>图1.2.1</p>
</div>

在神经网络中，激活函数的主要作用是引入非线性因素，解决线性模型不能解决的问题。上图中，为什么一定要做非线性变换而不直接输出$y=\alpha_{_{_1}}x_{{_1}}+\alpha_{_{_2}}x_{{_2}}+\alpha_{_{_3}}x_{{_3}}$呢？那是因为，线性输出$y$（也叫做预测值）会随着输入值的增大而无限增大，但有时候我们并不需要那么大的实际值，比如在做分类的时候，我们仅仅需要知道的是在某个阈值上下，$y$是为1（被激活）还是为0（未激活），通过判断1或者0就可以直接做出预测。另外，如果不用激励函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。如果使用激活函数的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。在上一节中，我们遇到一个特殊的情况，异或函数会让线性分类器无法把输入正确的归类，我们的解决办法是使用两个线性分类器，也许还有另一种办法，那就是使用圆把中间两个值圈起来，这种方法，其实就需要非线性变换。

激活函数在神经网络中的使用频率非常高，我们来看看常用的几个。

**阶跃函数**
为了模仿真正的生物神经元收到刺激后作出反应，我们在数学中首先想到的是阶跃函数。阶跃函数是在输入值未达到阈值后，输出一直为0，当输入达到阈值，输出值做突然的跳转，但这种冷冰冰，赤裸裸的尖锐边缘不太符合实际的应用，所以使用较少。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-10-40-25.png'>
<p style='text-align:center'>图1.2.2</p>
</div>

**Sigmoid函数**
相对于阶跃函数的尖锐突出，Sigmoid函数显得要平滑很多，无论它的输入值是无穷大还是无穷小，输出值范围均为$(0,1)$，很适合二分类问题，且输出的取值也很容易和概率取值联系起来。但是，Sigmoid也存在一些缺点，比如函数饱和使梯度消失，该函数的输出不是零中心的，这两个问题（后面梯度章节会讲解）在很多时候限制了它的使用。它的函数形式为：$$Sigmoid=\frac{1}{1+e^{-x}}$$
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-11-13-09.png'>
<p style='text-align:center'>图1.2.3</p>
</div>


<b>Tanh函数</b>
双曲正切函数跟Sigmoid函数很像，但是输出值范围扩大了，从-1到1，它也存在函数饱和问题，不过输出是零中心的，使得Tanh函数比Sigmoid函数更受欢迎。函数形式为$$Tanh=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$

<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-12-26-21.png'>
<p style='text-align:center'>图1.2.4</p>
</div>

<b>Softmax函数</b>
Softmax函数用于多分类网络中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，在最后输出时，选取概率最大的值作为预测值，从而来进行多分类！假设有一个数组，V，Vi表示V中的第i个元素，那么这个元素的softmax值就是：$$S_{_{i}}=\frac{e^{i}}{\sum_{j}^{e^{j}}}$$
用下图更清晰的表示Softmax函数的工作原理。三个输入经过Softmax的处理后，得到不同概率值，一般来说我们选取概率最大的值作为预测目标。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-17-15-07.png'>
<p style='text-align:center'>图1.2.4</p>
</div>

<b>Relu函数</b>
中文叫线性整流函数，又称修正线性单元，以斜坡函数为代表，当输入小于0时，输出为0，当输入大于0时，输出等于输入。该函数也是仿效生物神经元，最大的特点是可以避免梯度爆炸和梯度消失等问题，计算过程简单，因此，在深度神经网络中应用广泛。它的函数形式为:$$Relux=max(0,x)$$
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-18-14-18-51.png'>
<p style='text-align:center'>图1.2.6</p>
</div>

<a id="markdown-二线性回归与梯度下降" name="二线性回归与梯度下降"></a>
#二、线性回归与梯度下降
在上一节中我们普及了一次线性回归的概念，现在我们再来回顾一下，假定我们有一大批房屋面积和对应房价的信息，如果我们能得到房屋面积与房屋价格的线性关系，那么给定一个固定面积的房屋时，我们就能推算出其价格。对于这种预测问题我们的第一反应是使用线性回归模型来解决，回归模型定义了输入和输出的关系，输入现有房屋面积，预测房屋价格。

为了普遍性，我们把一个预测问题的步骤总结如下：
1、收集数据。在神经网络中，我们把数据称为训练样本，或训练集。
2、搭建模型，即搭建一个神经网络。
3、训练数据。让神经网络学习如何预测，得到完美的线性关系式。在学习的时候要有合适的指导方针，这种方针我们称为学习算法。
4、预测。学习完成后，使用新数据让模型得到预测输出。

收集数据不必细说，而模型我们知道是用类似$y=\alpha x+\beta $的线性模型，重点就是训练数据以及训练的指导方针，由此，我们不得不引入残差公式和梯度下降。

**残差公式**

线性模型可以用一条线性线段表示，在数学中线性线段为$y=\alpha x+\beta $，$x$代表宽度，$y$代表长度，$\alpha$和$\beta$分别为直线的斜率和截距。而在分类器中，我们把$x$称为输入，$y$称为输出，或者叫预测值，$\alpha$称为权重，$\beta$称为随机扰动项。如下图1.2.2所示的线性模型。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-24-17-09-49.png'>
<p style='text-align:center'>图1.2.7</p>
</div>

这样我们就把问题转化为分析两个变量$x$和$y$之间的关系，即分析$y=\alpha x+\beta$，比如一个程序员的收入$(x)$与支出$(y)$之间的关系，房子大小$(x)$与价格$(y)$之间的关系等等。

如上1.2.7的数据分布图，我们大概可以评估一下它们之间是一种线性的关系，但这种线性关系可以是多种，怎么才是最好的呢，那就要抓住关键点，即找出预测值和真实值之间的差异，并缩小这种差异。下图1.2.8我们可视化了这种差异，图中的$Δyi$称为误差或残差，是预测值和真实值之差的绝对值，用公式表示即为：$|Δy_{_i}|=|y'-y_{_i}|=|\alpha x_{_i}+\beta-y_{_i}|$，等式中$\alpha x_{_i}+\beta=y'$为预测值，$y_{_i}$为真实值。要想预测值越接近真实值，那么必然残差就越小，在已知$x$和$y$的情况下（$x$和$y$为收集到的训练数据），问题最后转化为求解$\alpha$、$\beta$值。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-24-17-56-59.png'>
<p style='text-align:center'>图1.2.8</p>
</div>

由于残差带有绝对值，计算起来比较麻烦，所以大家一致公认使用残差的平方，在多样本的情况下，上面的残差可以表示为$$\varepsilon _{i}^2=\frac{1}{2}\sum_{i=1}^{N}(\alpha_{i} x_{i}+\beta-y_{i})^2$$
这就是有名的残差公式，有时候也叫损失函数（Cost），只要能够使得损失函数值达到最小，那么问题就迎刃而解。

**梯度下降法**

现在我们先忽略多样本的情况，把这个让人凌乱的求和符号$\sum_{i=1}^{N}$给去掉，看看一个样本的情况。假设某个样本的残差为$\epsilon =(\alpha x+\beta-y)^2$（令$\epsilon=\varepsilon_{i}^2$），做$\alpha$和Є的坐标图如下1.2.9，这是一条曲线，形状好比一条峡谷，而且可以看到该曲线仅存在一个极小值点，只有在该点，才是峡谷的底部，Є的值才为最小。如何找到峡谷底部呢？图中，我们在曲线的任意位置（除极小值点）放置一颗小球，并让红色小球一步一步沿着“下坡路”往下滚，毫无疑问，当小球停止滚动的时候，也就是我们到达峡谷底部的时候。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-13-12-56-48.png'>
<p style='text-align:center'>图1.2.9</p>
</div>

如果用数学语言来描述的话，就是对$\alpha$求偏导！高数告诉我们偏导数即为某点的斜率，下图中红色小球的起点，即$\alpha_{_1}$点的斜率用绿色的箭头表示，该线段在$\alpha=\alpha_{_1}$点处与曲线相切，箭头反方向为小球滚动的方向。仔细看看这个图，只要我们沿着导数（斜率）相反的方向移动$\alpha$（对于$\beta$来说也是一样），就会找到极小值点了！
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-14-14-31-32.png'>
<p style='text-align:center'>图1.2.10</p>
</div>

上面的偏导$\frac{\partial \epsilon }{\partial \alpha }$我们也称为梯度，残差方程我们也称为损失函数，刚才讲的沿着导数的反方向向下移动寻找曲线极小值点的方法就叫梯度下降算法，这就好比我们所说的学习指导方针。在小球逐步往下移动的时候，每次移动的速率，我们用$\eta$表示，称作学习率，学习率的大小对于小球逼近极小值点有着至关重要的作用。较大的学习率会让小球错过极小值点，因为过大的学习率导致小球每次滚动的速率过大，在快要接近极小值点的时候却错过了极小值点。在实际应用中，这种情况会使我们的残差突然增大，来回摆动，梯度下降算法失效，所以我们通常会选取比较小的学习率，以避免逼近极小值点时来回震荡的情况。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-13-13-36-42.png'>
<p style='text-align:center'>图1.2.11</p>
</div>

通过梯度和学习率我们可以得到$\alpha$的新公式：$$\alpha_{_1}=\alpha_{_0}-\eta \frac{\partial \epsilon }{\partial \alpha}$$
延伸为一般的情况为：$$\alpha_{_n+1}=\alpha_{_n}-\eta \frac{\partial \epsilon }{\partial \alpha}$$

下面用一些具体值来说明梯度下降算法是如何工作的，首先为了简化计算，假设$\beta=0$，当前样本值为(x,y)=(1,0)，则损失函数表示为$\epsilon=(\alpha -1)^2$，参数$\alpha$的梯度为$\bigtriangledown =\frac{\partial \epsilon }{\partial \alpha }=2 (\alpha -1)$，那么使用梯度下降算法每次对参数$\alpha$的更新公式为$\alpha_{_n+1}=\alpha_{_n}-\eta \bigtriangledown n$，设置参数的初始值为5，学习率为0.3，那么这个优化过程可以总结为下表1.2.1。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-05-28-14-10-00.png'>
<p style='text-align:center'>表1.2.1</p>
</div>

可以看到，经过了10次迭代后，参数$\alpha$的值变成了1.00043，这个和参数最优值1已经比较接近了，虽然这里给出的是一个非常简单的样例，但是我相信你已经完全可以理解梯度下降算法的工作原理了。需要注意的是，梯度下降算法并不能保证被优化的函数达到全局最优解，即不保证小球能滚动到真正的谷底。如下图所示，山路弯曲，颠簸，凹凸不平，真正的谷底还未到，小球就停止了滚动。从数学上来解释，可以这样说，红色箭头处的斜率为0，梯度也为0，于是参数就停止更新。所以在实际应用中，我们必须保证损失函数为凸函数才能使用梯度下降算法，而只有在这种情况下梯度下降算法才能保证达到全局最优解。什么是凸函数？简单来理解就是曲线平滑，全局仅存在一个极小值点，不存在多个最小值点。（极小值点和最小值点是有区别的哦）
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-13-13-57-06.png'>
<p style='text-align:center'>图1.2.12</p>
</div>

<a id="markdown-三矩阵乘法" name="三矩阵乘法"></a>
#三、矩阵乘法
上面我们仅描述了一个输入的情况，即一个$x$对应一个$y$，而实际上，要进行二分类或者多分类，物体的特征值一定是多样的，否则无法达到分类的效果。就好比上面黄豆和蚕豆的例子，我们需要对其高、宽、颜色、重量等进行计算推导才能区别两种豆子。

现在，我们来演示一下把多输入转化为矩阵，并在模型中进行计算的过程。如下图所示，我们需要四个输入信号$x_{1},x_{2},x_{3},x_{4}$，和两个输出信号；每一个输入信号与输入层的每一个节点都有权重相连，可以看到$x_{1}$与输入层的第一个节点权重为$ω_{1,1}$，$x_{2}$和输入层的第一个节点间的权重为$ω_{2,1}$，$x_{2}$和输入层的第二个节点间的权重为$ω_{3,2}$等等，以此类推......

问题来了，一个线性模型，怎么会有多个输出信号呢？而且你也注意到了，我们在第一层输入层的后面我们加入了Sigmoid激活函数，这让模型看上去是两层神经网络，而线性模型仅有一层感知器才对！这里，我们当然需要解释一下，我们在此例中加入的激活函数并不增加一层网络深度，而仅仅是为了读者直观的感受而已！记住，无论在何种网络下，激活函数都不构成网络层数。所以，该模型也只是一层而已！同时，正是因为我们加入了Sigmoid激活函数，才会把线性模型的结果映射为两个输出，即我们把线性模型非线性化了。这种模型，我们称为逻辑回归。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-12-12-36-32.png'>
<p style='text-align:center'>图1.2.13</p>
</div>

理清思路后，我们继续往下看。
我们把多输入信号用例向量表示：
$$X=\begin{bmatrix}
x{_{1}}\\ 
x{_{2}}\\ 
x{_{3}}\\ 
x{_{4}}
\end{bmatrix}$$
权重用2X4维矩阵表示：
$$W=\begin{bmatrix}
w_{1,1} &w_{2,1}  &w_{3,1}  &w_{4,1} \\ 
w_{1,2} &w_{2,2}  &w_{3,2}  &w_{4,2} \\ 
\end{bmatrix}$$

第一层的第一个节点表示四个输入的加权和，第二个节点同理。那么，输出则可表示为输入和权重的点积，点积后的结果用$X_{output}$表示。
$$X_{output}=W\cdot X
=\begin{bmatrix}
w_{1,1} &w_{2,1}  &w_{3,1}  &w_{4,1} \\ 
w_{1,2} &w_{2,2}  &w_{3,2}  &w_{4,2} \\ 
\end{bmatrix} \cdot \begin{bmatrix}
x{_{1}}\\ 
x{_{2}}\\ 
x{_{3}}\\ 
x{_{4}}
\end{bmatrix}$$

即：$$X_{output}=
\begin{bmatrix}
X{_{1}}\\ X{_{2}}\\ 
\end{bmatrix}$$
其中：
$$X_{1}=w_{1,1}x{_{1}}+w_{2,1}x{_{2}}+w_{3,1}x{_{3}}+w_{4,1}x{_{4}}$$

$$X_{2}=w_{1,2}x{_{1}}+w_{2,2}x{_{2}}+w_{3,2}x{_{3}}+w_{4,2}x{_{4}}$$

得到了输入加权和以后，让该$X_{output}$通过$Sigmoid$激活函数得到最终的输出，用$O_{x}$表示：
$$O_{x}=Sigmoid(X_{output})=Sigmoid(\begin{bmatrix}
X{_{1}}\\ X{_{2}}\\ 
\end{bmatrix})$$

以上便是四个输入信号，两个输出的信号的计算过程。这个过程相对来说比较简单，如果有四个输出节点的话，那么权重为4X4维矩阵；如果中间还有隐藏层，那么每一层之间的权重又不一样了。
也许这样生硬的数学描述你可能会觉得眼花缭乱，不如我们设置一些具体的值来具体地计算一下，这样会有更好的理解。设输入
$$X=\begin{bmatrix}
5\\ 
3.5\\ 
1.4\\ 
0.2
\end{bmatrix}$$

权重$W$的值为随机选取值
$$W=\begin{bmatrix}
0.3 &0.6  &0.5  &0.8 \\ 
0.4 &0.2  &0.9  &0.7 \\ 
\end{bmatrix}$$

$$X_{output}=W\cdot X
=\begin{bmatrix}
0.3 &0.6  &0.5  &0.8 \\ 
0.4 &0.2  &0.9  &0.7 \\ 
\end{bmatrix} \cdot 
\begin{bmatrix}
5\\ 
3.5\\ 
1.4\\ 
0.2
\end{bmatrix}$$

$$O_{x}=\begin{bmatrix}
4.46\\ 
4.1\\ 
\end{bmatrix}$$

套用上面的点积运算公式可得结果，但我建议不要自己手动计算，使用计算机，让它发挥自己最好的计算优势。在之后的代码中，我们会使用Python的数学库numpy来进行大量的运算（以上公式使用numpy.dot()做点积运算），不论什么复杂计算，计算机的速度毫无疑问地完美超越人类。

在得到$X_{output}$后，我们把它输入到激活函数中。
$$Sigmoid(X_{output})=Sigmoid(\begin{bmatrix}
4.46\\ 
4.1\\ 
\end{bmatrix})$$

记住，复杂的计算仍然交给计算机。经过Sigmoid函数后，得到最后输出$O_{x}$。
$$O_{x}=\begin{bmatrix}
0.9885\\ 
0.9836\\ 
\end{bmatrix}$$

计算完成，至此，一组信号输入模型及最后输出的过程就全部清晰的展现出来了。矩阵乘法在神经网络中使用非常普遍，我们简单讲解一下应该很容易理解。不过，这一节引入了一个逻辑回归，那么逻辑回归和线性回归到底什么关系呢，衔接上下文，我们不难发现，其实逻辑回归就是在线性回归的基础上使用了激活函数。通过激活函数把输出映射成两个输出，然后使用两个输出做相应的预测，因此逻辑回归模型才是拿来做实际应用，而线性回归更常拿来做理论推导。

你也许会问还有一个重要的问题没有解决，就是权重如何优化？上面的矩阵乘法，权重都是事先设置好的，在实际训练中呢？当然，权重的优化离不开梯度下降算法的作用，而权重的初始值也是我们随机定义的，通过梯度下降，不断的更新权重，最后得到完美的线性模型。下一节我们会用一个实际的例子并附上代码来展示一次。
<div style='width:100px;hight:50px;margin-bottom:200px'></div>



参考文献：
1、《Python神经网络编程》
2、https://www.zhihu.com/question/22334626
3、https://zhuanlan.zhihu.com/p/21462488
4、https://www.zhihu.com/question/23765351
5、https://blog.csdn.net/ppn029012/article/details/8775597
6、《Tensorflow实战Google深度学习框架》
7、http://bookshadow.com/weblog/2014/06/10/precision-recall-f-measure/

