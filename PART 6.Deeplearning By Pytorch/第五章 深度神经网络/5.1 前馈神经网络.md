前面一章我们详细讲解了神经网络的组成，工作原理，信号在网络中如何流动，以及如何求解每一个输入信号赋予的权重等计算过程；同时我们还构建了一个逻辑回归网模型来解决鸢尾花分类问题，很明显，这种网络很“浅”，但它对于分类鸢尾花数据还是非常有效的，而且不仅仅是鸢尾花，对于有需要的其他二分类问题，该模型也能表现得很好。由于这种模型太“浅”了，我们一般称这种模型为bp网络，而不直接称为神经网络，有些人甚至觉得这种网络还不配叫做神经网络。我无需去争论这些芝麻小事，我们要做的就是从这一章起，迈向深度神经网络，了解什么是深度神经网络，它结构是什么样，它如何工作，以及综合前面三章的内容，用Pytorch搭建一个三层网络实现手写数字分类。

#1. 深度前馈网络
###1.1 什么是深度前馈网络
深度神经网络，简单来理解就是含有多个隐藏层的网络。一个深度神经网络总会有一个输入层，一个输出层，还有中间多个隐藏层，隐藏层的维数决定了网络的宽度。无论是输入层、隐藏层还是输出层，每一层都是由多个感知器组成，所以深度神经网络又称多层感知机。

前馈（feedforward）也可以称为前向，从信号流向来理解就是输入信号进入网络后，信号流动是单向的，即信号从前一层流向后一层，一直到输出层，其中任意两层之间的连接并没有反馈（feedback），亦即信号没有从后一层又返回到前一层。如果从输入输出关系来理解，则为当输入信号进入后，输入层之后的每一个层都将前一个层的输出作为输入。如下图所示的四层网络，这个图也可以称为有向无环路图。反之，当前馈神经网络中层与层之间的信号有反向流动，或者自输入时，我们则称这种网络为循环神经网络，循环神经网络在自然语言处理方面发挥着极大的作用。
![2018-06-13-18-02-29](http://qiniu.xdpie.com/2018-06-13-18-02-29.png)

在深度前馈网络中，链式结构也就是层与层之间的连接方式，层数就代表网络深度。如果我们把每一层看作一个函数，那么深度神经网络就是许多不同非线性函数复合而成，这里与之前典型的线性回归和逻辑回归明显的区别开来。比如，第一层函数为$f^{(1)}$，第二层函数为$f^{(2)}$，第三层函数为$f^{(3)}$，那么这个链式结构就可以表示为：$f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$，通过多次复合，实现输入到输出的复杂映射，链的全长也就代表模型深度。这种网络结构比较好搭建，应用也十分广泛，比如在图像识别领域占主导地位的卷积神经网络就是深度前馈网络的一种，学习这种网络，是我们通向循环神经网络的奠基石。

###1.2 深度学习
维基百科对深度学习的解释是：深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。由于深度神经网络也是多层非线性变换的载体，所以也有人认为深度学习就是深度神经网络的代名词。这里请注意，我们所说是深度神经网络，而不是深度前馈网络，前馈网络仅仅是深度神经网络的其中一种。
为什么深度学习是多层和非线性变换的结合呢，很显然，我们需要从两个方面来理解。
一，我们从之前的学习中可以知道线性模型仅仅能够解决的是简单的线性分类问题，对于异或逻辑的出现会直接让线性模型出现无法工作的情况，所以非线性变换随此出现。
二，对于上面提及的多层，其实我们指的是多层隐藏层。相对于输入层或输出层的设计直观性，隐藏层的设计即是科学又是艺术，一些经验法则指出，隐藏层并不是越多越好，神经网络的研究人员已经为隐藏层开发了许多设计最优法则，这有助于网络的行为能符合人们的期望。
如果把隐藏层看成一个黑盒，那么你所要关心的只是输出，而不关注隐藏层内部如何对数据特征进行提取，如何优化权重参数和随机变量；如果把隐藏层拆开细究，隐藏层代表的则是数据特征，上一个隐藏层把特征向量经过一系列变换后输入到下一个隐藏层，隐藏层的每一个神经元都带有特征数据向前传播。如下图举例说明，输入一张人物头像，三层隐藏层依次输出的图片信息可视化后，最后由计算机得出图像特征是人脸还是动物。
![2018-06-19-14-57-13](http://qiniu.xdpie.com/2018-06-19-14-57-13.png)
你可以这样理解，每一张图片就代表一层隐藏层的输出，这样我们便知道了隐藏层在对数据进行特征提取方面发挥着重要作用。
#2. 基于梯度的学习
###2.1 如何更新权重
到目前为止，我们了解的梯度下降算法是基于线性模型的残差$E(w)$，初始化权重$w$、可设定的常数学习率$\eta$的前提下，结合以下公式来不断更新。
$$w_{n+1}=w_{n}-\eta  \frac{\partial E(w)}{\partial w}$$
当然，还有种解法，如果残差（代价函数）为凸函数，那么我们只需要设残差（代价函数）的导数为0，便可以求得残差最小值所对应的权重。
$$E(w)'=0$$
这种方法理论上是可行的，但如果遇到代价函数的导数很难求解的时候，问题就卡住了，相信你应该不会硬碰硬的去直接求解函数$y=(e^x+x^2)^{\frac {1} {e^x+x}}$的导数吧？。因此，回到上面的$\frac{\partial E(w)}{\partial w}$，如果遇到导数无法求解的时候，我们是否可以换个方式或者法则来计算呢？数学中，链式法是求解复合函数导数的一种重要方法，让我们来回顾一下。
假设$E(f(w))$为$y=f(w)$和$w$复合而成的函数，那么对于求导数，我们可以有：
$$E(f(w))'=E'(f(w))f'(w)$$
对于求微分可写成:
$$\frac{\partial E}{\partial w}=\frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial w}$$
而我们面对的问题，和上面的链式法则定义一致。首先，别忘了残差$E$的定义是真实值$t$和输出预测值$O$之差的平方，表示为$E=(t-O)^2$
残差对权重的微分也可写作
$$\frac{\partial E}{\partial w}=\frac{\partial (t-O)^2}{\partial w}$$
而预测值$O$又是权重$w$的函数，$O=O(w)$

运用链式法则，我们把残差对权重的微分写成：
$$\frac{\partial E}{\partial w}=\frac{\partial E}{\partial O} \cdot \frac{\partial O}{\partial w}$$
其中$\frac{\partial E}{\partial O}$的微分是很容易计算的平方函数微分，所以上面的式子可为：
$$\frac{\partial E}{\partial w}=-2(t-O)\frac{\partial O}{\partial w}$$
进行到这里，我们可能需要引入深度前馈网络来详细考虑一下这个$\frac{\partial O}{\partial w}$的微分该如何表示。如下某网络中任意一层，其中有四个节点（每个节点为感知器或S型感知器），我们知道，无论这一层是输入层、隐藏层还是输出层，左边的输入来自外部输入或来自上一层网络，而右边的输入可作为下一层网络的输入，也可作为最终的输出。