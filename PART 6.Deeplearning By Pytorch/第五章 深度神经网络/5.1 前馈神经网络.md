前面一章我们详细讲解了神经网络的组成，工作原理，信号在网络中如何流动，以及如何求解每一个输入信号赋予的权重等计算过程；同时我们还构建了一个逻辑回归网模型来解决鸢尾花分类问题，很明显，这种网络很“浅”，但它对于分类鸢尾花数据还是非常有效的，而且不仅仅是鸢尾花，对于有需要的其他二分类问题，该模型也能表现得很好。由于这种模型太“浅”了，我们一般称这种模型为bp网络，而不直接称为神经网络，有些人甚至觉得这种网络还不配叫做神经网络。我无需去争论这些芝麻小事，我们要做的就是从这一章起，迈向深度神经网络，了解什么是深度神经网络，它结构是什么样，它如何工作，以及综合前面三章的内容，用Pytorch搭建一个三层网络实现手写数字分类。

#1. 深度前馈网络
###1.1 什么是深度前馈网络
深度神经网络，简单来理解就是含有多个隐藏层的网络。一个深度神经网络总会有一个输入层，一个输出层，还有中间多个隐藏层，隐藏层的维数决定了网络的宽度。无论是输入层、隐藏层还是输出层，每一层都是由多个感知器组成，所以深度神经网络又称多层感知机。

前馈（feedforward）也可以称为前向，从信号流向来理解就是输入信号进入网络后，信号流动是单向的，即信号从前一层流向后一层，一直到输出层，其中任意两层之间的连接并没有反馈（feedback），亦即信号没有从后一层又返回到前一层。如果从输入输出关系来理解，则为当输入信号进入后，输入层之后的每一个层都将前一个层的输出作为输入。如下图所示的四层网络，这个图也可以称为有向无环路图。反之，当前馈神经网络中层与层之间的信号有反向流动，或者自输入时，我们则称这种网络为循环神经网络，循环神经网络在自然语言处理方面发挥着极大的作用。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-13-18-02-29.png'>
<p style='text-align:center'>图5.1.1</p>
</div>

在深度前馈网络中，链式结构也就是层与层之间的连接方式，层数就代表网络深度。如果我们把每一层看作一个函数，那么深度神经网络就是许多不同非线性函数复合而成，这里与之前典型的线性回归和逻辑回归明显的区别开来。比如，第一层函数为$f^{(1)}$，第二层函数为$f^{(2)}$，第三层函数为$f^{(3)}$，那么这个链式结构就可以表示为：$f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$，通过多次复合，实现输入到输出的复杂映射，链的全长也就代表模型深度。这种网络结构比较好搭建，应用也十分广泛，比如在图像识别领域占主导地位的卷积神经网络就是深度前馈网络的一种，学习这种网络，是我们通向循环神经网络的奠基石。

###1.2 深度学习
维基百科对深度学习的解释是：深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。由于深度神经网络也是多层非线性变换的载体，所以也有人认为深度学习就是深度神经网络的代名词。这里请注意，我们所说是深度神经网络，而不是深度前馈网络，前馈网络仅仅是深度神经网络的其中一种。
为什么深度学习是多层和非线性变换的结合呢，很显然，我们需要从两个方面来理解。
一，我们从之前的学习中可以知道线性模型仅仅能够解决的是简单的线性分类问题，对于异或逻辑的出现会直接让线性模型出现无法工作的情况，所以非线性变换随此出现。
二，对于上面提及的多层，其实我们指的是多层隐藏层。相对于输入层或输出层的设计直观性，隐藏层的设计即是科学又是艺术，一些经验法则指出，隐藏层并不是越多越好，神经网络的研究人员已经为隐藏层开发了许多设计最优法则，这有助于网络的行为能符合人们的期望。
如果把隐藏层看成一个黑盒，那么你所要关心的只是输出，而不关注隐藏层内部如何对数据特征进行提取，如何优化权重参数和随机变量；如果把隐藏层拆开细究，隐藏层代表的则是数据特征，上一个隐藏层把特征向量经过一系列变换后输入到下一个隐藏层，隐藏层的每一个神经元都带有特征数据向前传播。如下图举例说明，输入一张人物头像，三层隐藏层依次输出的图片信息可视化后，最后由计算机得出图像特征是人脸还是动物。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-19-14-57-13.png'>
<p style='text-align:center'>图5.1.2</p>
</div>

你可以这样理解，每一张图片就代表一层隐藏层的输出，这样我们便知道了隐藏层在对数据进行特征提取方面发挥着重要作用。
#2. 梯度下降的再次学习
到目前为止，我们了解的梯度下降算法是基于线性模型的残差$E(w)$，初始化权重$w$、可设定的常数学习率$\eta$的前提下，结合以下公式来不断更新。
$$w_{n+1}=w_{n}-\eta  \frac{\partial E(w)}{\partial w}$$
当然，还有种解法，如果残差（代价函数）为凸函数，那么我们只需要设残差（代价函数）的导数为0，便可以求得残差最小值所对应的权重。
$$E(w)'=0$$
这种方法理论上是可行的，但如果遇到代价函数的导数很难求解的时候，问题就卡住了，相信你应该不会硬碰硬的去直接求解函数$y=(e^x+x^2)^{\frac {1} {e^x+x}}$的导数吧？。因此，回到上面的$\frac{\partial E(w)}{\partial w}$，如果遇到导数无法求解的时候，我们是否可以换个方式或者法则来计算呢？数学中，链式法是求解复合函数导数的一种重要方法，让我们来回顾一下。
假设$E(f(w))$为$y=f(w)$和$w$复合而成的函数，那么对于求导数，我们可以有：
$$E(f(w))'=E'(f(w))f'(w)$$
对于求微分可写成:
$$\frac{\partial E}{\partial w}=\frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial w}$$
而我们面对的问题，和上面的链式法则定义一致。首先，别忘了残差$E$的定义是真实值$t{k}$和输出预测值$O_{k}$之差的平方，表示为$E=(t_{k}-O_{k})^2$
残差对权重的微分也可写作
$$\frac{\partial E}{\partial w}=\frac{\partial (t-O)^2}{\partial w}$$
而预测值$O_{k}$又是权重$w$的函数，$O_{k}=O_{k}(w)$

运用链式法则，我们把残差对权重的微分写成：
$$\frac{\partial E}{\partial w_{j,k}}=\frac{\partial E}{\partial O_{k}} \cdot \frac{\partial O_{k}}{\partial w}$$
其中$\frac{\partial E}{\partial O}$的微分是很容易计算的平方函数微分，所以上面的式子可为：
$$\frac{\partial E}{\partial w_{j,k}}=-2(t_{k}-O_{k})\frac{\partial O_{k}}{\partial w_{j,k}}$$
进行到这里，我们可能需要引入深度前馈网络来详细考虑一下这个$\frac{\partial O_{k}}{\partial w_{j,k}}$的微分该如何表示。如下某网络中任意一层，其中有四个节点，每个节点为S型感知器（即加入Sigmoid函数），我们知道，无论这一层是输入层、隐藏层还是输出层，这一层左边的输入来自外部输入或来自上一层网络的输出，而层右边的输出可作为下一层网络的输入，也可作为最终的输出。接着，我们用$w_{j,k}$来表示上一层的第$j$个节点和如图所示的这一层第$k$个节点的链接权重（其中$j=1,2...n$，$k=1,2...m$)。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-20-22-02-22.png'>
<p style='text-align:center'>图5.1.3</p>
</div>

当输入进入该层，权重和输入信号进行加权求和，同时通过Sigmoid函数输出值：
$$O_{k}=Sigmoid({\sum_{k=1,j=1}^{4}} w_{j,k} \cdot x_{j})$$

这样推导过后，我们可以得到下面的式子：
$$\frac{\partial E}{\partial w_{j,k}}=-2(t_{k}-O_{k})\frac{\partial}{\partial w_{j,k}} Sigmoid(\sum w_{j,k} \cdot x_{j})$$
在求解Sigmoid函数的导数前我们回顾一下$Sigmoid$函数表达式：
$$Sigmoid(x)=\frac{1}{1+e^{-x}}$$
$Sigmoid$对$x$的微分：
$$\frac{\partial }{\partial x}Sigmoid(x)=Sigmoid(x)(1-Sigmoid(x))$$
我们暂时不必把$Sigmoid$函数带入计算，从这里可以看出原来Sigmoid函数的导数是如此简单，而且易于使用。

在求解$\frac{\partial}{\partial w_{j,k}} Sigmoid(\sum w_{j,k})$，再使用一次链式法则。

我们得出一个非常炫酷的式子：
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-21-21-48-41.png'>
<p style='text-align:center'></p>
</div>

这确实是一个振奋人心的结果，眼尖的你可能一眼就看出表达式中每一项代表的意思。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-21-18-04-33.png'>
<p style='text-align:center'></p>
</div>

不过我觉得把式子中的"2"去掉也许更简洁，为什么？因为不管前面常数是-2，2，或者是20都不是影响残差斜率的关键因素，我们只需要记住自己需要什么就可以了，这样残差的斜率就被我们简化成最干净利落的样子：
$$\frac{\partial E}{\partial w_{j,k}}=-(t_{k}-O_{k}) \cdot O_{k}\cdot (1-O_{k}) \cdot x_{j}$$

我们可以放心大胆的在线性感知器中使用权重更新公式了，只要我们记住权重改变的方向与梯度方向相反，以及适度的设置其中的学习率以防止超调。
$$w_{n+1}=w_{n}-\eta \frac{\partial E}{\partial w_{j,k}}$$

不难看出，新的权重$w_{n+1}$是由刚刚得到误差斜率取反来调整旧的权重$w_{n}$而得到的。如果斜率为正，我们希望减小权重，如果斜率为负，我们希望增加权重，因此，我们要对斜率取反。学斜率$\eta$就是用于调节这些变化的强度，确保不会超调。现在终于可以明白，我们在上一章鸢尾花的分类中定义权重时一个重要的初始化步骤竟然就是这样来的。
```python
self.wih += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)),
                                        numpy.transpose(inputs))
```
这个权重更新表达式不仅适用于隐藏层和输出层之间的权重，而且适用于输入层和隐藏层之间的权重。比如鸢尾花分类例子中使用的模型只有一层，类似上图5.1.3，因此，代码中```final_outputs```代表$O_{k}$，而```inputs```则就是外部输入$x_{j}$；若模型为多层网络，那么"$O_{k}$"代表某一层的输出，$x_{j}$代表某一层的输入。到此为止，梯度下降算法总算了解透彻，如果还有其他复杂的数学推导放一边去吧，还有什么不满足的呢，用上面的公式，以及结合python语言，我们已经能够快速实现模型的期望结果。

不知你发现了没有，当输入向量$X$输入感知器时，第一次初始化权重向量$W$是随机组成的，也可以理解成我们任意设置了初始值，并和输入做点积运算，然后模型通过权重更新公式来计算新的权重值，更新后的权重值又接着和输入相互作用，如此迭代多次，得到最终的权重。

信号向前传播，权重的更新反向传播，是这样吗？
是的，你的直觉没错，确实是反向传播。

#3. 反向传播算法
##3.1 前馈的实质
反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习。此外，反向传播经常被误解为仅适用于多层神经网络，但是原则上它可以计算任何函数的导数（对于一些函数，正确的响应是报告函数的导数是未定义的）。

不如我们还是使用上一节矩阵乘法的列子来可视化的讲解，这样有助于我们去理解反向传播。假设有如下三层网络，输入层、隐藏层、输出层，现有一组信号$X$输入网络，输入层和隐藏层的链接权重$W_{input-hidden}$和隐藏层与输出层之间的权重$W_{hidden-ouput}$我们随机初始化。为了清晰效果，我们仅标注了几个权重，第一个输入节点和中间隐藏层第一个节点之间的权重为$w_{1,1}$ = 0.9，正如上图中的神经网络所示。同样，你可以看到输入的第二节点和隐藏层的第二节点之间的链接的权重为$w_{2,2}$ = 0.8，隐藏层第三个节点和输出层第二个节点之间链接的权重为$w_{3,2}$ = 0.2......此命名方式前面解释过，标注之后在分析反向传播时可帮助我们理解。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-28-15-50-17.png'>
<p style='text-align:center'>图5.1.5</p>
</div>

输入矩阵：
$$X=\begin{bmatrix}
0.9\\ 
0.1\\ 
0.8\end{bmatrix}$$
输入层和隐藏层之间的连接权重：
$$W_{input-hidden}=\begin{bmatrix}
0.9 &  0.3&0.4 \\ 
 0.2&  0.8&0.2 \\ 
 0.8&  0.1&0.9 
\end{bmatrix}$$
隐藏层和输出层之间的连接权重：
$$W_{hidden-output}=\begin{bmatrix}
0.3 &  0.7&0.5 \\ 
 0.6&  0.5&0.2 \\ 
\end{bmatrix}$$
初始值定义好以后，开始计算输入到隐藏层的组合调节输入值$X_{hidden}$。
$$X_{hidden} = W_{input_hidden} \cdot X$$
此处的矩阵乘法还是交由计算机来执行，计算出的答案如下：
$$X_{hidden} =\begin{bmatrix}
0.9 &  0.3&0.4 \\ 
 0.2&  0.8&0.2 \\ 
 0.8&  0.1&0.9 
\end{bmatrix} \cdot \begin{bmatrix}
0.9\\ 
0.1\\ 
0.8\end{bmatrix}$$
$$X_{hidden} =\begin{bmatrix}
1.16\\ 
0.42\\ 
0.62\end{bmatrix}$$
别着急往下走，让我们来整理一下网络的信号流动情况，$X_{hidden}$作为第一层的输出，第二层的输入已经正确求解，现在它准备进入隐藏层。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-28-16-09-22.png'>
<p style='text-align:center'>图5.1.6</p>
</div>

$X_{hidden}$一进入隐藏层，我们就对$X_{hidden}$的这些节点使用S激活函数，使其变得更加自然，并且我们把经过S函数处理后的这组输出信号命名为$O_{hidden}$。
$$O_{hidden}=Sigmoid(X_{hidden})=Sigmoid(\begin{bmatrix}
1.16\\ 
0.42\\ 
0.62\end{bmatrix})=\begin{bmatrix}
0.761\\ 
0.603\\ 
0.650\end{bmatrix}$$

暂停一下，让我们再次可视化这些输入到第二层隐藏层的组合调节输入。现在信号已经向前流动到了第二层，下一步当然是计算第三层的输出信号$X_{output}$（还未经过S函数的输出信号），计算的方法和前面一样，没有什么区别，不管我们的网络是几层，这种方法都适用。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-28-16-03-21.png'>
<p style='text-align:center'>图5.1.7</p>
</div>

于是，我们有：
$$X_{output}=W_{hidden-output} \cdot O_{hidden}=\begin{bmatrix}
0.3 &  0.7&0.5 \\ 
 0.6&  0.5&0.2 \\ 
\end{bmatrix} \cdot 
\begin{bmatrix}
0.761\\ 
0.603\\ 
0.650\end{bmatrix}=
\begin{bmatrix}
0.975\\ 
0.888\\ 
\end{bmatrix}$$
现在，更新示意图展示我们的进展，从初始输入信号开始，一层层往前流动的前馈信号，最后得到了最终层的组合输入信号。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-28-16-14-47.png'>
<p style='text-align:center'>图5.1.8</p>
</div>

最后一步当然是使用S函数得到最后一层的输出，用$O_{ouput}$表示： 
$$O_{ouput}=Sigmoid(X_{output})=Sigmoid(
\begin{bmatrix}
0.975\\ 
0.888\\ 
\end{bmatrix}
)=
\begin{bmatrix}
0.726\\ 
0.708\\ 
\end{bmatrix}
$$

前馈信号的流动到此为止，任务完成！通过可视化的图形，前馈神经网络中信号流入方向，变化等情况我们用网络图最后形展示出来。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-06-28-16-33-36.png'>
<p style='text-align:center'>图5.1.9</p>
</div>

毫无疑问，整个过程就是前馈的意思，信号一直向前流动，最后输出，中间任意层没有信号反回上一级网络。
下一步我们会将神经网络的输出值与训练样本中的输出值进行比较，计算出误差，并使用这个误差值来反向调节权重值。

##3.2 反向传播的实质

上一步我们得到了前向传播的输出值为[0.726, 0.708]，这个值与真实值[0.01，0.99]还存在一定差距，不过没关系，反向传播误差会帮助我们更新权值，缩小这些误差，让我们来实验一下。

**1. 计算总误差**

因为总误差为：$E=\sum(target-O_{output})^2=E_{1}+E_{2}=(target1-O_{output1})^2+(target2-O_{output2})^2$
由于我们的实验网络有两个输出，因此总误差为两个输出误差之和。
第一个误差：
$$E_{1}=(target_{1}-O_{output1})^2=(0.726-0.01)^2=0.512656$$
第二个误差：
$$E_{2}=(target_{2}-O_{output2})^2=(0.706-0.99)^2=0.079524$$
总误差：
$$E=E_{1}+E_{2}=0.512656+0.079524=0.59218$$

**2. 隐藏层和输出层的权重更新**

对于隐藏层和输出层之间的权重$w_{1,1}$来说，如果我们想知道$w_{1,1}$对整体误差产生了多少影响，可以用总误差对$w_{1,1}$求偏导，该偏导可以使用链式法则表示。
$$\frac{\partial E}{\partial w_{1,1}}=\frac{\partial E}{\partial O_{ouput1}} \cdot \frac{\partial O_{ouput1}}{\partial X_{ouput1}} \cdot \frac{\partial X_{ouput1}}{\partial w_{1,1}}$$
如图所示的反向传播示意图，并结合求导表达式，可以帮助我们更清楚的了解误差是怎么反向传播的。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-01-15-23-04.png'>
<p style='text-align:center'>图5.1.10</p>
</div>

下面我们对以上求导式子中的每个小式子分别求值
1、首先是计算$\frac{\partial E}{\partial O_{ouput1}} $
$$E=(target_{1}-O_{output1})^2+(target2-O_{output2})^2$$
$$\frac{\partial E}{\partial O_{ouput1}}=-2(target_{1}-O_{output1})+0=-2(0.01-0.726)=1.432$$
2、再来计算$\frac{\partial O_{ouput1}}{\partial X_{ouput1}}$
$$O_{ouput1}=\frac{1}{1+e^{-X_{ouput1}}}$$
$$\frac{\partial O_{ouput1}}{\partial X_{ouput1}}=O_{ouput1}(1-O_{ouput1})=0.726(1-0.726)=0.198924$$
3、最后计算$\frac{\partial X_{ouput1}}{\partial w_{1,1}}$
$$X_{ouput1}=w_{1,1} \cdot O_{hidden1}+w_{2,1} \cdot O_{hidden2}+w_{3,1} \cdot O_{hidden3}$$
$$\frac{\partial X_{ouput1}}{\partial w_{1,1}}=O_{hidden1}=0.761$$
所以：
$$\frac{\partial E}{\partial w_{1,1}}=\frac{\partial E}{\partial O_{ouput1}} \cdot \frac{\partial O_{ouput1}}{\partial X_{ouput1}} \cdot \frac{\partial X_{ouput1}}{\partial w_{1,1}}=1.432 \times 0.198924 \times 0.761=0.216777826848$$
我们取学习率$\eta=0.5$，利用公式$${w_{1,1}}_{new}=w_{1,1}-\eta \frac{\partial E}{\partial w_{1,1}}$$
得到更新后的$w_{1,1}$为：$${w_{1,1}}=0.3-0.5 \times 0.216777826848=0.191611086576$$
综上所述，也可以这样去计算$\frac{\partial E}{\partial w_{1,1}}$：
$$\frac{\partial E}{\partial w_{1,1}}=-2(target_{1}-O_{output1}) \cdot O_{ouput1}(1-O_{ouput1}) \cdot O_{hidden1}$$
因此，改变上述式子的变量可以更新$w_{2,1}$，$w_{2,1}$，$w_{1,2}$，$w_{2,2}$，$w_{3,2}$等权重值。

**3. 输入层和隐藏层的权重更新**
计算输入层和隐藏层之间的权重和上面的方法一样，但使用误差对权重进行求导时，该误差应使用两个输出口的总误差，而不是一个输入口的误差。我们仍然用图形化的方式来展示：
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-01-20-39-16.png'>
<p style='text-align:center'>图5.1.11</p>
</div>

如上图所示，对于隐藏层和输出层之间的权重$w_{1,1}$来说，如果我们想知道$w_{1,1}$对整体误差产生了多少影响，可以用总误差对$w_{1,1}$求偏导，该偏导可以使用链式法则表示。
$$\frac{\partial E}{\partial w_{1,1}}=\frac{\partial E}{\partial O_{hidden1}} \cdot \frac{\partial O_{hidden1}}{\partial X_{hidden1}} \cdot \frac{\partial X_{hidden1}}{\partial w_{1,1}}$$
我们还是一个一个的计算上面的式子。
1、首先计算$\frac{\partial E}{\partial O_{hidden1}}$
对于隐藏层的输出，它会接受来自两个输出传来的误差，所以：
$$\frac{\partial E}{\partial O_{hidden1}}=\frac{\partial E_{1}}{\partial O_{hidden1}}+\frac{\partial E_{2}}{\partial O_{hidden1}}$$
$$\because \frac{\partial E_{1}}{\partial O_{hidden1}}=\frac{\partial E_{1}}{\partial X_{output1}} \cdot \frac{\partial X_{output1}}{\partial O_{hidden1}}$$
$$\because \frac{\partial E_{1}}{\partial X_{output1}}=\frac{\partial E_{1}}{\partial O_{output1}} \cdot \frac{\partial O_{output1}}{\partial X_{output1}}=1.437 \times 0.198924=0.285853788$$
下面的$w'_{j,k}$为隐藏层和输出层的链接权重
$$X_{output1}=w'_{1,1} \cdot O_{hidden1}+w'_{2,1} \cdot O_{hidden2}+w'_{3,1} \cdot O_{hidden3}$$
$$\therefore \frac{\partial X_{output1}}{\partial O_{hidden1}}=w'_{1,1}=0.3$$
$$\therefore \frac{\partial E_{1}}{\partial O_{hidden1}}=\frac{\partial E_{1}}{\partial X_{output1}} \cdot \frac{\partial X_{output1}}{\partial O_{hidden1}}=0.285853788 \times 0.3=0.0857561364$$
再来计算$\frac {\partial E_{2}}{\partial O_{hidden1}}$
$$\because \frac{\partial E_{2}}{\partial O_{hidden1}}=\frac{\partial E_{2}}{\partial X_{output2}} \cdot \frac{\partial X_{output2}}{\partial O_{hidden1}}$$
$$\because \frac{\partial E_{2}}{\partial X_{output2}}=\frac{\partial E_{2}}{\partial O_{output2}} \cdot \frac{\partial O_{output2}}{\partial X_{output2}}$$
$w'_{j,k}$为隐藏层和输出层的链接权重
$$X_{output2}=w'_{1,2} \cdot O_{hidden1}+w'_{2,2} \cdot O_{hidden2}+w'_{3,2} \cdot O_{hidden3}$$
$$\therefore \frac{\partial X_{output2}}{\partial O_{hidden1}}=w'_{1,2}$$
$$\therefore \frac{\partial E_{2}}{\partial O_{hidden1}}=\frac{\partial E_{2}}{\partial X_{output2}} \cdot \frac{\partial X_{output2}}{\partial O_{hidden1}}=-0.116599104 \times 0.2=-0.0233198208$$
最后得到
$$\frac{\partial E}{\partial O_{hidden1}}=\frac{\partial E_{1}}{\partial O_{hidden1}}+\frac{\partial E_{2}}{\partial O_{hidden1}}=0.0857561364-0.0233198208=0.0624363156$$
2、再计算$\frac{\partial O_{hidden1}}{\partial X_{hidden1}}$
$$\because O_{hidden1}=\frac{1}{1+e^{-X_{hidden1}}}$$
$$\frac{\partial O_{hidden1}}{\partial X_{hidden1}}=O_{hidden1}(1-O_{hidden1})=0.761(1-0.761)=0.181879$$
3、最后计算$\frac{\partial X_{hidden1}}{\partial w_{1,1}}$
$$\because X_{hidden1}=w_{1,1} \cdot X_{1}+w_{2,1} \cdot X_{2}+w_{3,1} \cdot X_{3}$$
$$\therefore \frac{\partial X_{hidden1}}{\partial w_{1,1}}=X1=0.9$$
$$\frac{\partial E}{\partial w_{1,1}}=\frac{\partial E}{\partial O_{hidden1}} \cdot \frac{\partial O_{hidden1}}{\partial X_{hidden1}} \cdot \frac{\partial X_{hidden1}}{\partial w_{1,1}}=0.0624363156 \times 0.181879 \times 0.9=0.01022026918051116$$
我们取学习率$\eta=0.5$，利用公式$${w_{1,1}}_{new}=w_{1,1}-\eta \frac{\partial E}{\partial w_{1,1}}$$
得到更新后的$w_{1,1}$为：$${w_{1,1}}=0.9-0.5 \times 0.01022026918051116=0.191611086576=0.89488986540974442$$
同样的方法可以更新其他权重的值。这样我们就完成了误差反向传播算法的介绍，在实际训练中我们通过这种方法不停的迭代，直到总误差接近0为止，得到的最优权重保留下来，训练完成。
