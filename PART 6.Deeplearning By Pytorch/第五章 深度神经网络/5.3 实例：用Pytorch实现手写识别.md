手写数字识别常常被当做学习神经网络类似“Hello World”入门级的实例，瞧瞧下面一排数字，人类大脑可以不费吹灰之力就可以认出这些数字为“504192”，这个识别可以说几乎是无意识的，人类在理解展示在眼睛面前的信息非常擅长，而我们对于自己能够很好的处理各种视觉信息深信不疑。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-03-19-38-10.png'>
<p style='text-align:center'>图5.3.1</p>
</div>

但是如果你让计算机来识别上面的数字，就会发现非常困难，看起来人类一下子就能完成的任务变得几乎不可能。就拿数字“9”来说，它由一个圈和右下方一条竖线或曲线段组成，就这样一个简单的视觉，实际上算法都很难轻易表达出来。而在你试着让计算机的识别规则越发精准时，你就会很掉进一个深渊中，没有出路，感到毫无希望，甚至绝望。

不过，神经网络以另一种方式看待这个问题。其主要思想是获取大量的手写数字，常称作训练样本（如下图），然后开发出一个可以从这些训练样本中进行学习的系统。换言之，神经网络使用样本来自动推断出识别手写数字的规则。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-03-19-49-18.png'>
<p style='text-align:center'>图5.3.2</p>
</div>

另外，通过增加训练样本的数量，网络可以学到更多关于手写数字的知识，这样就能够提升自身的准确性。所以，上面例子中我们只是展出了 100 个训练数字样本，而通过使用数千或者数百万或者数十亿的训练样本我们也许能够得到更好的手写数字识别器。

手写数字识别作为经典案例实际并不复杂，我们使用的代码也仅仅是纯python语言，没有使用Pytorch。我们首先会搭建一个三层网络，包含一层输入层、一层隐藏层、一层输出层，并利用上几章学到的方法来训练和调试参数，三层网络还不算深度神经网络，所以，紧接着我们使用同样的方法再增加一层隐藏层，即使用四层神经网络来训练数据，观察增加网络深度对模型的准确率有什么影响，以及把模型增加为五层网络又会对模型准确率造成什么影响等。好了，让我们开始吧。
#1. 搭建神经网络
##1.1 三层网络架构
开始试验前，请下载我们帮你准备好的MNIST数据，下载地址为https://pan.baidu.com/s/1vm4hPfCeJXDQqihbuo7HnQ

训练数据集为.csv格式，包含60000个样本, 测试数据集包含10000个样本。在MNIST数据集中的每张图片由28 x 28个像素点构成，每个像素点用一个灰度值表示。在这里，我们将28 x 28的像素展开为一个一维的行向量，每行784个值, 一行代表一张图片。因此，无论是搭建一个三层网络或者四层网络或多层网络，输入层的神经元个数为784个，每一个神经元接收一张图片的像素值。同理，由于我们的手写数字只有0~9十个数字，所以输出层有10个神经元，每个神经元代表一个数字。

直观的输入层和输出层定义好以后，重点就是搭建隐藏层，隐藏层神经元个数的设置即是一种艺术，又是一种科学，我们很难在这里用长篇大论的文字来描述为什么一定就要设置100个，或者200个神经元。所以，抛开疑虑，尽管跟着经验或经典之路吧！此处，我们在隐藏层设置200个神经元。这个网络如下图所示：
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-04-17-03-37.png'>
<p style='text-align:center'>图5.3.3</p>
</div>

784个输入节点，200个隐藏点，算一算，输入层和隐藏层之间的链接权重有784 X 200 = 156800个！隐藏层和输出层之间的链接权重有200 X 10 = 2000个！而整个训练和调参过程计算机上只花费了不过几分钟的时间而已！这没什么大惊小怪的，我们都没有使用到超级复杂的解决方法和超大规模的计算资源。后续的卷积网络和循环神经网络，或许才会给计算机带来一些挑战。

##1.2 训练网络
搭建整个网络我们仅用了72行代码，有些代码与之前鸢尾花分类使用的类似，比如输入和权重的点积运算，使用Sigmoid激活函数，学习率同为0.01等等。但鸢尾花分类使用的模型仅有一层网络，而该网络有三层，重点是我们有两处权重值需要调节。还记得之前我们分析并简化了的损失函数斜率和权重更新公式吗？结合代码再来回顾一下，对于梯度下降算法我们用最原始的方式写出来，让自己更清晰的了解，原来，无论现今有多少著名的框架，也无论神经网络有几层，梯度下降算法不过如此。
$$\frac{\partial E}{\partial w_{j,k}}=-(t_{k}-O_{k}) \cdot O_{k}\cdot (1-O_{k}) \cdot x_{j}$$
$$w_{n+1}=w_{n}-\eta  \frac{\partial E(w)}{\partial w_{j,k}}$$
```python

# update the weights for the links between the input and hidden layers
        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)),numpy.transpose(inputs))

# update the weights for the links between the hidden and output layers
        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)),
                                        numpy.transpose(hidden_outputs))

```

训练该网络仍采用闭环形式，输入训练数据$\rightarrow$预测结果$\rightarrow$计算损失函数$\rightarrow$调整模型参数$\rightarrow$输入训练数据，如此循环迭代5次，最后把调整后的最优权值保存下来供验证使用。
```python

# epochs is the number of times the training data set is used for training
epochs = 5

for e in range(epochs):
    # go through all records in the training data set
    print('start training, epoch:', e)
    for record in training_data_list:
        # split the record by the ',' commas
        all_values = record.split(',')
        # scale and shift the inputs
        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01
        # create the target output values (all 0.01, except the desired label which is 0.99)
        targets = numpy.zeros(output_nodes) + 0.01
        # all_values[0] is the target label for this record
        targets[int(all_values[0])] = 0.99
        n.train(inputs, targets)
        pass

    numpy.save('../model/wih.npy', n.wih)
    numpy.save('../model/who.npy', n.who)
    pass

```

##1.3 验证模型
验证模型时，输出层最后的得到的是一个10维数组，每个数组值对应的索引值就表示预测数字。最后我们得到了超过95%的准确率值，这已经很不错了，人类所能达到的准确值也不过97%左右！
```python
performance =  0.9546

```
你不会就这样停止不前，得到这个结果后，你马上就想发掘一项技术让准确率达到99%，或者至少可以在95.46%的基础上把准确率提升一点，是的，我想这是所有人的心声！一定是有办法的，等等，刚刚我们在训练数据的时候仅仅迭代了5次，增加训练次数当然可以提高模型的准确率，这是最简单的办法了！
设置```epoch=10```，经过CPU半小时的疯狂计算，得到的准确率为：
```python
performance =  0.9695

```
我们把迭代次数调到10次，模型的准确率就提高到了96.95%，如果把迭代次数增加为50次或者100次，模型的准确率毫无疑问可以再提高，但至于提高到哪个数值，我们没有继续尝试了。这种方法简单易实现，我们不再讨论，现在我们重点来看看增加网络深度对模型准确率的影响。

#2. 使用深度神经网络来训练
##2.1 搭建多层神经网络
增加一层隐藏层其实并不是一项复杂的工作，同样的神经元个数，同样的训练参数，视觉上网络看起来更繁杂了，训练时间也加长了，但得到的效果却是令人兴奋的。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-10-14-33-40.png'>
<p style='text-align:center'>图5.3.4</p>
</div>

四层神经网络的代码的如下，增加的隐藏层神经元个数也为200个，同时初始化第一层隐藏层和第二层隐藏层之间的权重。
```python
class deepNeuralNetwork:
    # initialise the neural network
    def __init__(self, inputnodes, hidden1nodes, hidden2nodes, outputnodes, learningrate):
        # set number of nodes in each input, hidden, output layer
        self.inodes = inputnodes
        self.h1nodes = hidden1nodes
        self.h2nodes = hidden2nodes
        self.outnodes = outputnodes

        self.weight_h1 = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.h1nodes, self.inodes))
        self.weight_h2 = numpy.random.normal(0.0, pow(self.h1nodes, -0.5), (self.h2nodes, self.h1nodes))
        self.weight_out = numpy.random.normal(0.0, pow(self.h2nodes, -0.5), (self.outnodes, self.h2nodes))

```

训练的时候需要特别注意每一层的输入输出关系，以及采用梯度下降算法更新权重时残差斜率的公式是否使用了正确的输入输出值。
```python
    def train(self, inputs_list, targets_list):
        # convert inputs list to 2d array
        inputs = numpy.array(inputs_list, ndmin=2).T
        targets = numpy.array(targets_list, ndmin=2).T

        # calculate signals into first hidden layer
        hidden1_inputs = numpy.dot(self.weight_h1, inputs)
        # calculate the signals emerging from hidden layer
        hidden1_outputs = self.activation_function(hidden1_inputs)

        # calculate signals into second hidden layer
        hidden2_inputs = numpy.dot(self.weight_h2, hidden1_outputs)
        # calculate the signals emerging from hidden layer
        hidden2_outputs = self.activation_function(hidden2_inputs)

        # calculate signals into final output layer
        final_inputs = numpy.dot(self.weight_out, hidden2_outputs)
        # calculate the signals emerging from final output layer
        final_outputs = self.activation_function(final_inputs)

        # output layer error is the (target - actual)
        output_errors = targets - final_outputs

        # hidden layer error is the output_errors, split by weights, recombined at second hidden nodes
        hidden2_errors = numpy.dot(self.weight_out.T, output_errors)

        # hidden layer error is the output_errors, split by weights, recombined at first hidden nodes
        hidden1_errors = numpy.dot(self.weight_h2.T, hidden2_errors)

        # update the weights for the links between the hidden and output layers
        self.weight_out += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)),
                                               numpy.transpose(hidden2_outputs))
        # update the weights for the links between the first hidden layers and second hidden layers
        self.weight_h2 += self.lr * numpy.dot((hidden2_errors * hidden2_outputs * (1.0 - hidden2_outputs)),
                                              numpy.transpose(hidden1_outputs))
        # update the weights for the links between the input and first hidden layers
        self.weight_h1 += self.lr * numpy.dot((hidden1_errors * hidden1_outputs * (1.0 - hidden1_outputs)),
                                              numpy.transpose(inputs))

        pass

```
第一次训练，我们使用的迭代次数为5次，准确率96.9%；第二次训练迭代10次，准确率97.61%，我们非常乐观的判定增加网络深度会提升网络模型的准确率。
如此，我们再增加一层隐藏层，使得整个网络为5层架构（此处省略代码），通过5次迭代训练，准确率为96.23%，比四层架构的网络降低了一点点；如果再增加一层隐藏层，即使用6层神经网络架构，准确率仅为95.8%，与三层网络架构模型的准确率差不多。这种结果让人匪夷所思，因为我们直觉地认为，额外的隐藏层应当让网络能够学到更加复杂的分类函数，然后可以在分类时表现得更好，但事实又与我们想象的相反。

其实，这些都源于“深度”问题，更直接地说是因为出现梯度消失或梯度爆炸。这并不是我研究出来的，这里我们仅仅是引用一下别人的研究成果来丰富我们的内心。

##2.2 梯度消失和梯度爆炸
###2.2.1 隐藏层学习速率
Michael Nielsen的《神经网络与深度学习》一书中详细描述了产生梯度消失和梯度爆炸的原因，我们首先看看在神经网络中隐藏层中每一层的学习速率，为了可视化这种情况，Michael给大家画了一幅四层网络的部分神经元网络图：
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-11-11-58-49.png'>
<p style='text-align:center'>图5.3.5</p>
</div>

图中的每个神经元有一个条形统计图，表示每个神经元权重和偏差在神经网络学习时的变化速率。大的条意味着更快的速度，而小的条则表示变化缓慢。我们可以发现，第二个隐藏层上的条基本上都要比第一个隐藏层上的条要大。所以，在第二个隐藏层的神经元将学习得更加快速。那么如果是第三层隐藏层呢？Michael用了下面几幅图告诉大家答案。
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-11-12-04-26.png'>
</div>
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-11-12-04-46.png'>
<p style='text-align:center'>图5.3.6</p>
</div>

从上图可以看出，后一层隐藏层学习速率均高于前一层，甚至第四层的学习速率为第一层的100倍！这就是为什么我们之前在训练5层或6层神经网络时遇到了大麻烦。
那么我们是否可以得出一个结果：在某些深度神经网络中，隐藏层BP（反向传播）的时候梯度倾向于变小，这个现象被称作梯度消失。
###2.2.2 梯度消失
为何消失的梯度问题会出现呢？我们可以通过什么方式避免它？还有在训练深度神经网络时如何处理好这个问题？我们一步一步地来弄清楚，首先来看看一个极简单的深度神经网络：每一层都只有一个单一的神经元。下图就是有三层隐藏层的神经网络：
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-12-10-59-48.png'>
<p style='text-align:center'>图5.3.7</p>
</div>

这里$w_{_1},w_{_2},... $是权重，$b_{_1},b_{_2},... $是偏置，$E$是残差（代价函数）。第$j$个隐藏神经元的输出可用式$O_{hidden_{j}}=\sigma(X_{hidden_{j}})$表示，输出层的输出用$O_{output}$表示，$\sigma$是通常的$Sigmoid$函数，而$X_{hidden_{j}}=w_{j}x_{j}+b_{j}$是隐藏层神经元的带权输入。现在我们来研究一下关联于第一个隐藏神经元梯度$∂E/∂b_{_1}$表达式，通过研究表达式来理解消失的梯度发生的原因。
利用链式法则可推导代价函数对于偏置$b_{_1}$的梯度为：
$$\frac{\partial E}{\partial b_{1}}=\frac{\partial E}{\partial O_{ouput}} \cdot \frac{\partial O_{output}}{\partial X_{output}} \cdot \frac{\partial X_{output}}{\partial O_{hidden3}} \cdot \frac{\partial O_{hidden3}}{\partial X_{hidden3}} \cdot \frac{\partial X_{hidden3}}{\partial O_{hidden2}} \cdot \frac{\partial O_{hidden2}}{\partial X_{hidden2}} \cdot \frac{\partial X_{hidden2}}{\partial O_{hidden1}} \cdot \frac{\partial O_{hidden1}}{\partial X_{hidden1}} \cdot \frac{\partial X_{hidden1}}{\partial b_{1}}$$
回顾上一节的反向传播推导过程，我们直接得出式子：
$$\frac{\partial E}{\partial b_{1}}=\frac{\partial E}{\partial O_{ouput}} \cdot \sigma'(X_{hidden_{4}}) \cdot w_{4} \cdot \sigma'(X_{hidden_{3}}) \cdot w_{3} \cdot \sigma'(X_{hidden_{2}}) \cdot w_{2} \cdot \sigma'(X_{hidden{1}})$$
该表达式除第一项和最后一项以外，全是$\sigma'(X_{hidden_{j}}) \cdot w_{j}$的式子，再来看看$Sigmoid$函数的导数，即$\sigma'(X_{hidden_{j}})$的函数图形（引用$Michael$的图片）：
<div align=center>
<img src='http://qiniu.xdpie.com/2018-07-12-11-14-45.png'>
<p style='text-align:center'>图5.3.1</p>
</div>


可以看到，$Sigmoid$函数的导数最大不超过0.25，即$\frac{1}{4}$，如果我们采用标准的初始化权重的方法，也就是一般情况下，我们会使初始化的权重$-1<w_{j}<1$这个范围内，所以，当层数越深，代价函数对于第一层隐藏层参数的梯度会由很多项$\sigma'()w$的乘积组成，当这些项都小于1时，在我们进行了所有这些项的乘积后，最终结果肯定会指数级下降：项越多，乘积的下降的越快。这其实就是梯度消失出现的本质原因。

###2.2.3 梯度爆炸
梯度爆炸跟梯度下降的实质是相同的，对于同样的式子，如果我们把权值初始化变得很大——超过1，那么我们将不再遇到消失的梯度问题，这时候梯度会在我们BP（反向传播）的时候发生指数级地增长。也就是说，我们遇到了梯度爆炸的问题。

让我们来试试梯度爆炸是如何出现的，比如，我们将网络的权重设置得很大，使得$|\sigma'()w|>1$，并且是远大于1，这样我们可以发现所有$|\sigma'()w|$项相乘的结果都元大于1，甚至呈指数级增长，最终，我们就获得了爆炸的梯度。

奉上$Michael$的总结性语言。“对于不稳定的梯度问题，根本的问题其实并非是消失的梯度问题或者爆炸的梯度问题，而是在前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡。如果没有某种机制或者更加本质的保证来达成平衡，那网络就很容易不稳定了。简而言之，真实的问题就是神经网络受限于不稳定梯度的问题。所以，如果我们使用标准的基于梯度的学习算法，在网络中的不同层会出现按照不同学习速度学习的情况。”

###2.2.4 梯度消失/爆炸的解决方案

梯度消失的问题本质上是由于多项$\sigma'()w$相乘的结果造成的，从源头上讲，一个是激活函数，一个是权重值。由于Sigmoid函数的导数最大值不超过0.25，所以慎重选择激活函数会对梯度产生质的影响。另外对权重的处理也是一项必不可少的工作，以下我们总结了一些方法。
**1 使用relu、leakrelu、elu等激活函数**
*relu：*
导数为1，只要权值得当，那就不会导致梯度消失或梯度爆炸问题；
计算方便，训练速度快；
但由于relu函数负数部分恒为0，会导致一些神经元无法激活；
输出不是以0为中心的。

*leakrelu：*
数学表达式为$leakrelu=max(kx,0)$，其中k为leak系数，可以取0.01或0.02，也可以通过学习得到。leakrelu解决了relu函数负数部分恒为0的情况。leakrelu导数为K,同时计算速度也快。

*elu:*
数学表达式为$
\begin{cases}
 x& \  x>0 \\ 
 \alpha (e^x-1))& \ otherwise 
\end{cases}
$，也是为解决relu负数部分为0而生，只是计算速度较慢。

**2 权重正则化**
权重正则化是针对梯度爆炸问题而提出的。我们来看看正则项在代价函数的形式： 
$$E=(y-w^Tx)^2 + \alpha||w||^2$$
α 是指正则项系数，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。

**3 残差结构和LSTM**
这两种方式实际上已经超出了向前传播的范围，属于后面我们会讲的循环神经网络部分，这里省略。

总而言之，梯度消失和梯度爆炸仅仅是深度学习中众多问题之一，本章中，我们基于BP公式推导得到了梯度不稳定性的结论，清楚了激活函数的选择，权重的初始化，甚至是学习算法的实现方式在深度网络的训练中均扮演了十分重要的角色。当然，网络结构和其他超参数本身也是很重要的，因此，太多因子影响了训练神经网络的难度，搞清楚这些因子并采取办法改进是我们研究的重点，我们还有很长的一段路要走。




























<div style='width:100px;hight:50px;margin-bottom:300px'></div>
参考文献:
1、《神经网络与深度学习》
2、https://blog.csdn.net/qq_25737169/article/details/78847691