# 03 Tensorflow 逻辑回归

## 3.1 逻辑回归原理
### 3.1.1 逻辑回归
在现实生活中，我们遇到的数据大多数都是非线性的，因此我们不能用上一章线性回归的方法来进行数据拟合。但是我们仍然可以从线性模型着手开始第一步，首先对输入的数据进行加权求和。
**线性模型**：
$$z=w\bold{x}+b$$
其中w我们称为“权重”，b为偏置量（bias），$\bold{x}$为输入的样本数据，三者均为向量的形式。

我们先在二分类中来讨论，假如能创建一个模型，如果系统输出1，我们认为是第一类，如果系统输出0，我们认为是第二类，这种输出需求有点像阶跃函数（海维塞德阶跃函数），但是阶跃函数是间断函数，y的取值在x=0处突然跳跃到1，在实际的建模中，我们很难在模型中处理这种情况，所以使用Sigmoid函数。
![2017-09-06-12-04-48](http://qiniu.xdpie.com/2017-09-06-12-04-48.png)

**Sigmoid函数**：
$$y=sigmoid(z)=\frac{1}{1+{e}^{-z}}$$
Sigmoid函数是激活函数其中的一种，当x=0时，函数值为0.5，随着x的增大，对应的Sigmoid值趋近1，而随着x的减小，Sigmoid值趋近0。通过这个函数，我们可以得到一系列0—1之间的数值，接着我们就可以把大于0.5的数据分为1类，把小于0.5的数据分为0类。

![2017-09-05-15-24-33](http://qiniu.xdpie.com/2017-09-05-15-24-33.png)


这种方式等价于是一种概率估计，我们把y看作服从伯努利分布，在给定x条件下，求解每个$y_i$为1或0的概率。此时，逻辑回归这个抽象的名词，在这里我们把它转化成了能够让人容易理解的概率问题。接着通过最大对数似然函数估计w值，就解决问题了。
$y_i$等于1的概率为：$$sigmoid(w{x_i}+b)$$
$y_i$等于0的概率为：$$1-sigmoid(w{x_i}+b)$$

以上对Sigmoid函数描述可以看出该函数多用于二分类，而我们会经常遇到多分类问题，这时，Softmax函数的价值就体现出来了。

**Softmax函数**：
Softmax函数也是激活函数的一种，主要用于多分类，把输入的线性模型当成幂指数求值，最后把输出值归一化为概率，通过概率来把对象分类，而每个对象之间是不相关的，所有的对象的概率之和为1。对于Softmax函数，如果j=2的话，Softmax和Sigmoid是一样的，同样解决的是二分类问题，这时用两种函数都能进行很好的二分类。
$$softmax(z)_i=\frac{e^{z_i}}{\sum_{j}{e^{z_j}}}$$
以上公式可以理解为，样本为类别$i$的概率。即：
$$y_{_i}=softmax(\bold{w}\bold{x}+\bold{b})=\frac{e^{w{x_i}+b}}{\sum_{j}{e^{w{x_j}+b}}}$$

对于Softmax回归模型的解释，在这里引用一下别人的图，一张图片就胜过千言万语。

![2017-09-05-17-51-03](http://qiniu.xdpie.com/2017-09-05-17-51-03.png)

如果写成多项式，可以是这样：

![2017-09-05-17-53-51](http://qiniu.xdpie.com/2017-09-05-17-53-51.png)

如果换成我们常用的矩阵的形式，可以是这样：

![2017-09-05-17-58-51](http://qiniu.xdpie.com/2017-09-05-17-58-51.png)

### 3.1.2 成本函数
在线性回归中，我们定义了一个由和方差组成的损失函数，并使该函数最小化来找到$\theta$的最优解。同样的，在逻辑回归中我们也需要定义一个函数，通过最小化这个函数来解得我们的权重w值和偏差b值。在机器学习中，这种函数可以看做是表示一个模型的好坏的指标，这种指标可以叫做成本函数（Cost）或损失函数（Loss），然后最小化这两z种函数，但是这两种方式都是一样的。
这里介绍一个常见的成本函数“交叉熵”，在后面的实例代码中我们会用到。交叉熵产生于信息论里面的信息压缩编码技术，后来慢慢演变成从博弈论到机器学习等其他领域的重要技术，它用来衡量我们的预测用于描述真相的低效性。它的定义如下：
$$H_{{_y}'}(y)=-\sum_{i}{y_{_i}'log(y_{_i})}$$
它是怎么推导出来的呢，我们先来讨论一下Sigmoid的损失函数，接着再来对比理解。在上面我们知道了$y_i$等于1的概率为：
$$P(y_i=1|x_i)=\frac{1}{1+e^{wx_{_i}+b}}$$
$y_i$等于0的概率为：
$$P(y_i=0|x_i)=1-\frac{1}{1+e^{wx_{_i}+b}}$$
概率密度函数为：
$$P(y_i|x_i)=(\frac{1}{1+e^{wx_{_i}+b}})^{y_i}({1-\frac{1}{1+e^{wx_{_i}+b}}})^{1-{y_i}}$$
接着我们取对数似然函数，然后最小化似然函数进行参数估计（这里省略似然函数和一系列文字）。
而我们把问题泛化为多分类时，同样可以得出我们的概率密度函数：
$$P(y|x)=\prod_iP(y_i|x)^{y_i}$$
我们对概率密度取对数的负数，就得到了我们的似然函数，即我们这里称为交叉熵的函数，其中$\bold{y}$是预测的概率分布，$\bold{y}'$是实际的概率分布。
$$-\sum_{i}{y_{_i}'log(y_{_i})}=-\sum_{i}{y_{_i}'log(softmax(wx+b))}$$
通过最小化该交叉熵，找出最优化的w和b值。

