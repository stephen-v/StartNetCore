# 4 Tensorflow支持向量机
## 4.1 SVM介绍
SVM（Support Vector Machines）——支持向量机是在所有知名的数据挖掘算法中最健壮，最准确的方法之一。假设在一个可线性二分的数据集中，图一A所示，我们要找到一个超平面把两组数据分开，这时，我们认为线性回归的直线或逻辑回归的直线也能够做这个分类，这条直线可以是图一B中的直线，也可以是图一C中的直线，或者图一D中的直线，但哪条直线才最好呢，也就是说哪条直线能够达到最好的泛化能力呢？那就是一个能使两类之间的空间大小最大的一个超平面。
这个超平面在二维平面上看到的就是一条直线，在三维空间中就是一个平面...，因此，我们把这个划分数据的决策边界统称为超平面。离这个超平面最近的点就叫做支持向量，点到超平面的距离叫间隔。支持向量机就是要使超平面和支持向量之间的间隔尽可能的大，这样超平面才可以将两类样本准确的分开，而保证间隔尽可能的大就是保证我们的分类器误差尽可能的小，尽可能的健壮。
![2017-09-07-13-58-24](http://qiniu.xdpie.com/2017-09-07-13-58-24.png)
图一


## 4.2 工作原理
### 4.2.1 几何间隔和函数间隔
在最大化支持向量到超平面距离前，我们首先要定义我们的超平面$g({\bold{x}})$（称为超平面的判别函数，也称给定$\bold{w}$和$\bold{b}$的泛函间隔），其中$\bold{w}$为权重向量，$\bold{b}$为偏移向量：
$$g({\bold{x}})=\bold{w}^T\bold{x}+\bold{b}$$
样本$\bold{x}$到最优超平面的**几何间隔**为：
$$r=\frac{g(\bold{x})}{||\bold{w}||}=\frac{\bold{w}^T\bold{x}+\bold{b}}{||\bold{w}||}$$
$||\bold{w}||$是向量$\bold{w}$的内积，是个常数，即$||\bold{w}||=\sqrt{{w_{_0}}^2+{w_{_1}}^2+...+{w_{_n}}^2}$，而$|g({\bold{x}})|$就是下面要介绍的函数间隔。

**函数间隔**：
$$\widehat{r}=g({\bold{x}})$$
函数间隔$|g(\bold{x})|$是对线性模型求绝对值，从而保证间隔为正数，但它是一个并不标准的间隔度量，是人为定义的，它不适合用来做最大化的间隔值，因为，一旦超平面固定以后，如果我们人为的放大或缩小$\bold{w}$和$\bold{b}$值，那这个超平面也会无限的放大或缩小，这将对分类造成严重影响。而几何间隔是函数间隔除以$||\bold{w}||$，当$\bold{w}$的值无限放大或缩小时，$||\bold{w}||$也会放大或缩小，而整个$r$保持不变，它只随着超平面的变动而变动，不受两个参数的影响。因而，我们用几何间隔来做最大化间隔度量。
### 4.2.2 最大化间隔
回想我们的Sigmoid函数，当输入一个线性模型$\bold{w}\bold{x}+\bold{b}$，输出1或者0，并以此来判断样本是属于类别1还是类别0。在支持向量机中，我们会把几何间隔$\bold{r}$输入到一个类似Sigmoid的函数中，然后再输出-1或1。什么采用-1和+1，而不是0和1呢？这是由于-1和+1仅仅相差一个符号，方便数学上的处理。我们可以通过一个统一公式来表示间隔或者数据点到分隔超平面的距离，同时不必担心数据到底是属于-1还是+1类。
我们一步一步的进行分析，首先如下图，在这个$\mathbb{R}^2$空间中，假设我们已经确定了一个超平面，这个超平面的函数关系式应该是$g({\bold{x}})=\bold{w}^T\bold{x}+\bold{b}=0$，这个式子表示我们图中的那条虚线，很明显，这个式子意思是说点x在超平面上，但我们要想使所有的点都尽可能的远离这个超平面，我们只要保证离这个超平面最近的点远离这个超平面，也就是说这些叫支持向量的点$x^*$需要尽可能的远离它，就可以保证所有的点都远离这个超平面了。

![2017-09-08-10-26-04](http://qiniu.xdpie.com/2017-09-08-10-26-04.png)

我们把其中一个支持向量$x^*$到最优超平面的距离定义为：
$$r^*={\frac{g(\bold{x}^*)}{||\bold{w}||}}= \left\{\begin{matrix}
{\frac{1}{||\bold{w}||}} & if:y^*=g(\bold{x}^*)=+1 \\ 
& \\ 
{-\frac{1}{||\bold{w}||}} & if:y^*=g(\bold{x}^*)=-1
\end{matrix}\right.$$

这是我们通过一个类似Sigmoid函数的作用使得输出$y=g(\bold{x})=\pm1$而得来的。我们可以把这个式子想象成还存在两个平面，这些支持向量就在这两个平面上，这两个平面离最优超平面的距离越大，我们的间隔也就越大。对于其他的点如果满足下面的条件，那分类就很明显了，即如果数据点处于超平面负方向（-1类）并且离分隔超平面很远的位置时，被分为一类，反之则分为另一类。
$$\left\{\begin{matrix}
{\bold{w}^T\bold{x_{_i}}+\bold{b}}\geqslant 1 & if:y_{_i}=+1 \\ 
& \\ 
{\bold{w}^T\bold{x{_i}}+\bold{b}} \leqslant 1& if:y_{_i}=-1
\end{matrix}\right.$$

支持向量到超平面的距离知道后，那么分离的间隔$\rho$很明显就为：
$$\rho=2r^*=\frac{2}{||\bold{w}||} $$
这下我们就要通过找到最优的$\bold{w}$和$\bold{b}$来最大化$\rho$了，感觉又像回到了逻辑回归或线性回归的例子。但是这里，我们最大化$\rho$值需要有条件限制，即：
$$\begin{cases}
 & \max \limits_{\bold{w},\bold{b}} {\frac{2}{||\bold{w}||}} \\ 
 &  \\ 
 & \bold{y_{_i}}(\bold{w}^T\bold{x_{_i}}+\bold{b}) \geqslant 1, \ (i=1,..,n)
\end{cases}$$
同时，为了计算方便，我们把上式最大化$\rho$换成：
$$\begin{cases}
 & \min \limits_{\bold{w},\bold{b}} {\frac{1}{2}}||\bold{w}||^2 \\ 
 &  \\ 
 & \bold{y_{_i}}(\bold{w}^T\bold{x_{_i}}+\bold{b}) \geqslant 1, \ (i=1,..,n)
\end{cases}$$

这种式子通常我们用拉格朗日乘数法来求解：
$$L(\bold{w},\bold{b},\alpha)=\frac{1}{2}\bold{w}^T\bold{w}-\sum_{i=1}^{n}\alpha_{_i}[y_{_i}(\bold{w}^T\bold{x_{_i}}+\bold{b})-1] $$
其中$\alpha_{i}$称为拉格朗日乘子。这样，我们的问题就转化为通过最小化$L(\bold{w},\bold{b},\alpha)$，来求得$\bold{w}$和$\bold{b}$，从而确定最优超平面。
对$L(\bold{w},\bold{b},\alpha)$的$\bold{w}$和$\bold{b}$求偏导并令其等于0：
$$\begin{cases}
 & \frac{\partial L(\bold{w},\bold{b},\alpha )}{\partial \bold{w}} =0\\ 
 & \ \\ 
 & \frac{\partial L(\bold{w},\bold{b},\alpha )}{\partial \bold{b}}=0
\end{cases}$$
得：
$$\begin{cases}
 & \bold{w}=\sum_{i=1}^{n}\alpha{_i}y_{_i}x_{_i} \\ 
 & \ \\ 
 & \sum_{i=1}^{n}\alpha{_i}y_{_i}=0
\end{cases}$$
把该式代入上面的拉格朗日式子可得：
$$\max \limits_{\alpha}W(\alpha)=\sum_{i=1}^{n}\alpha{_i}-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha{_i}\alpha{_j}y_{_i}y_{_j}{x_{_i}}^Tx_{_j} $$
$$
\sum_{i=1}^{n}\alpha{_i}y_{_i}=0  \ , \alpha_{_i}\geqslant 0 (i=1,...,n)$$
这是一个
## 4.3 SMO算法
## 4.4 核函数
## 4.5 实例